#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RG.ru çˆ¬è™« - å¸¦å¼‚å¸¸ä¸­æ–­é‡å¯åŠŸèƒ½
"""
import requests
from bs4 import BeautifulSoup, Tag
import json
import os
from datetime import datetime, timedelta
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from time import sleep
import re
import warnings
import logging
import shutil
import subprocess
import platform
import random
import traceback
import time
import glob

# å°è¯•å¯¼å…¥webdriver_managerï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
try:
    from webdriver_manager.chrome import ChromeDriverManager

    WEBDRIVER_MANAGER_AVAILABLE = True
except ImportError:
    WEBDRIVER_MANAGER_AVAILABLE = False
    print("âš ï¸ webdriver-manageræœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")

TXT_FILE = 'rg_ru_titles.txt'
JSON_DIR = 'data'

# å…¨å±€å˜é‡ï¼Œè®°å½•ä¸Šä¸€ä¸ªæœ‰æ•ˆæ—¥æœŸ
last_valid_date = None

warnings.filterwarnings("ignore")
logging.getLogger("selenium").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("requests").setLevel(logging.ERROR)

MONTH_MAP = {
    'ÑĞ½Ğ²Ğ°Ñ€Ñ': '01', 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ': '02', 'Ğ¼Ğ°Ñ€Ñ‚Ğ°': '03', 'Ğ°Ğ¿Ñ€ĞµĞ»Ñ': '04', 'Ğ¼Ğ°Ñ': '05', 'Ğ¸ÑĞ½Ñ': '06',
    'Ğ¸ÑĞ»Ñ': '07', 'Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°': '08', 'ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ': '09', 'Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ': '10', 'Ğ½Ğ¾ÑĞ±Ñ€Ñ': '11', 'Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ': '12',
    'ÑĞ½Ğ²': '01', 'Ñ„ĞµĞ²': '02', 'Ğ¼Ğ°Ñ€': '03', 'Ğ°Ğ¿Ñ€': '04', 'Ğ¸ÑĞ½': '06', 'Ğ¸ÑĞ»': '07', 'Ğ°Ğ²Ğ³': '08', 'ÑĞµĞ½': '09', 'Ğ¾ĞºÑ‚': '10',
    'Ğ½Ğ¾Ñ': '11', 'Ğ´ĞµĞº': '12'
}

# å¼‚å¸¸è®¡æ•°å™¨
exception_count = 0
MAX_EXCEPTION_RETRY = 5
EXCEPTION_COOLDOWN = 60  # å¼‚å¸¸åå†·å´æ—¶é—´ï¼ˆç§’ï¼‰


def find_chromedriver():
    """æŸ¥æ‰¾ç³»ç»Ÿä¸­å·²å®‰è£…çš„ChromeDriver"""
    possible_paths = []

    # Windowsè·¯å¾„
    if platform.system() == "Windows":
        possible_paths.extend([
            "chromedriver.exe",
            "C:\\chromedriver\\chromedriver.exe",
            "C:\\Program Files\\chromedriver\\chromedriver.exe",
            "C:\\Program Files (x86)\\chromedriver\\chromedriver.exe",
            os.path.join(os.getcwd(), "chromedriver.exe"),
            os.path.join(os.path.dirname(__file__), "chromedriver.exe")
        ])
    else:
        # Linux/Macè·¯å¾„
        possible_paths.extend([
            "chromedriver",
            "/usr/local/bin/chromedriver",
            "/usr/bin/chromedriver",
            "/opt/chromedriver/chromedriver",
            os.path.join(os.getcwd(), "chromedriver"),
            os.path.join(os.path.dirname(__file__), "chromedriver")
        ])

    # æ£€æŸ¥PATHç¯å¢ƒå˜é‡
    try:
        result = subprocess.run(['chromedriver', '--version'],
                                capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            return "chromedriver"  # åœ¨PATHä¸­æ‰¾åˆ°
    except:
        pass

    # æ£€æŸ¥å¯èƒ½çš„è·¯å¾„
    for path in possible_paths:
        if os.path.exists(path):
            return path

    return None


def download_chromedriver_manual():
    """æ‰‹åŠ¨ä¸‹è½½ChromeDriverçš„å¤‡ç”¨æ–¹æ¡ˆ"""
    import urllib.request
    import zipfile

    # æ ¹æ®ç³»ç»Ÿç¡®å®šä¸‹è½½URL
    system = platform.system().lower()
    machine = platform.machine().lower()

    if system == "windows":
        if "64" in machine:
            url = "https://chromedriver.storage.googleapis.com/LATEST_RELEASE"
            # è·å–æœ€æ–°ç‰ˆæœ¬å·
            try:
                with urllib.request.urlopen(url) as response:
                    version = response.read().decode('utf-8').strip()
                download_url = f"https://chromedriver.storage.googleapis.com/{version}/chromedriver_win32.zip"
            except:
                # ä½¿ç”¨å›ºå®šç‰ˆæœ¬
                download_url = "https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_win32.zip"
        else:
            download_url = "https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_win32.zip"
    elif system == "linux":
        download_url = "https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip"
    elif system == "darwin":  # macOS
        if "arm" in machine:
            download_url = "https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_mac_arm64.zip"
        else:
            download_url = "https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_mac64.zip"
    else:
        return None

    try:
        print(f"ğŸ”§ æ­£åœ¨æ‰‹åŠ¨ä¸‹è½½ChromeDriver: {download_url}")

        # åˆ›å»ºä¸‹è½½ç›®å½•
        download_dir = os.path.join(os.getcwd(), "chromedriver_download")
        os.makedirs(download_dir, exist_ok=True)

        # ä¸‹è½½æ–‡ä»¶
        zip_path = os.path.join(download_dir, "chromedriver.zip")
        urllib.request.urlretrieve(download_url, zip_path)

        # è§£å‹æ–‡ä»¶
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(download_dir)

        # æ‰¾åˆ°chromedriverå¯æ‰§è¡Œæ–‡ä»¶
        for root, dirs, files in os.walk(download_dir):
            for file in files:
                if file.startswith('chromedriver'):
                    chromedriver_path = os.path.join(root, file)
                    # è®¾ç½®æ‰§è¡Œæƒé™ï¼ˆLinux/Macï¼‰
                    if system != "windows":
                        os.chmod(chromedriver_path, 0o755)
                    return chromedriver_path

        return None
    except Exception as e:
        print(f"âŒ æ‰‹åŠ¨ä¸‹è½½ChromeDriverå¤±è´¥: {e}")
        return None


def get_local_chrome_version():
    """è‡ªåŠ¨æ£€æµ‹æœ¬åœ°Chromeä¸»ç‰ˆæœ¬å·ï¼ˆä»…æ”¯æŒWindowsï¼‰"""
    import winreg
    chrome_reg_paths = [
        r"SOFTWARE\Google\Chrome\BLBeacon",
        r"SOFTWARE\WOW6432Node\Google\Chrome\BLBeacon"
    ]
    for reg_path in chrome_reg_paths:
        try:
            reg_key = winreg.OpenKey(winreg.HKEY_CURRENT_USER, reg_path)
            version, _ = winreg.QueryValueEx(reg_key, "version")
            winreg.CloseKey(reg_key)
            return version
        except:
            pass
        try:
            reg_key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, reg_path)
            version, _ = winreg.QueryValueEx(reg_key, "version")
            winreg.CloseKey(reg_key)
            return version
        except:
            pass
    # å¤‡é€‰ï¼šå°è¯•é€šè¿‡chrome.exe --version
    try:
        result = subprocess.run([
            r"C:\Program Files\Google\Chrome\Application\chrome.exe", "--version"
        ], capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            m = re.search(r"(\d+\.\d+\.\d+\.\d+)", result.stdout)
            if m:
                return m.group(1)
    except:
        pass
    return None


def get_chromedriver_path():
    """è·å–ChromeDriverè·¯å¾„ï¼Œä¼˜å…ˆè‡ªåŠ¨æ£€æµ‹æœ¬åœ°Chromeä¸»ç‰ˆæœ¬å·å¹¶ä¸‹è½½å¯¹åº”ç‰ˆæœ¬"""
    # æ–¹æ¡ˆ1: æŸ¥æ‰¾ç³»ç»Ÿä¸­å·²å®‰è£…çš„ChromeDriver
    chromedriver_path = find_chromedriver()
    if chromedriver_path:
        print(f"âœ… æ‰¾åˆ°å·²å®‰è£…çš„ChromeDriver: {chromedriver_path}")
        return chromedriver_path

    # æ–¹æ¡ˆ2: è‡ªåŠ¨æ£€æµ‹æœ¬åœ°Chromeä¸»ç‰ˆæœ¬å·å¹¶ä¸‹è½½å¯¹åº”ç‰ˆæœ¬
    if WEBDRIVER_MANAGER_AVAILABLE:
        chrome_version = get_local_chrome_version()
        if chrome_version:
            main_version = chrome_version.split('.')[0]
            print(f"ğŸ” æ£€æµ‹åˆ°æœ¬åœ°Chromeä¸»ç‰ˆæœ¬å·: {main_version}")
            sources = [
                ("é»˜è®¤æº", None),
                ("é˜¿é‡Œäº‘", 'https://registry.npmmirror.com/-/binary/chromedriver'),
                ("æ¸…åæº", 'https://mirrors.tuna.tsinghua.edu.cn/chromedriver/')
            ]
            for name, mirror in sources:
                try:
                    print(f"ğŸ”§ webdriver-managerå°è¯•ä¸‹è½½ChromeDriverï¼ˆ{name}ï¼Œç‰ˆæœ¬{main_version}ï¼‰...")
                    if mirror is None:
                        os.environ.pop('WDM_MIRROR', None)
                    else:
                        os.environ['WDM_MIRROR'] = mirror
                    os.environ['WDM_CACHE_PATH'] = os.path.abspath('./chromedriver_cache')
                    os.environ['WDM_LOCAL'] = '0'
                    os.environ['WDM_SSL_VERIFY'] = 'false'
                    chromedriver_path = ChromeDriverManager(driver_version=main_version).install()
                    print(f"âœ… webdriver-managerï¼ˆ{name}ï¼‰ä¸‹è½½æˆåŠŸ: {chromedriver_path}")
                    return chromedriver_path
                except Exception as e:
                    print(f"âŒ webdriver-managerï¼ˆ{name}ï¼‰ä¸‹è½½å¤±è´¥: {e}")
        else:
            print("âš ï¸ æœªèƒ½è‡ªåŠ¨æ£€æµ‹åˆ°æœ¬åœ°Chromeç‰ˆæœ¬ï¼Œå°è¯•é€šç”¨æ–¹å¼ä¸‹è½½...")
            # ç»§ç»­åç»­é€»è¾‘
        # å…¼å®¹åŸæœ‰é€»è¾‘ï¼šå°è¯•ä¸æŒ‡å®šç‰ˆæœ¬çš„ä¸‰æº
        sources = [
            ("é»˜è®¤æº", None),
            ("é˜¿é‡Œäº‘", 'https://registry.npmmirror.com/-/binary/chromedriver'),
            ("æ¸…åæº", 'https://mirrors.tuna.tsinghua.edu.cn/chromedriver/')
        ]
        for name, mirror in sources:
            try:
                print(f"ğŸ”§ å°è¯•ä½¿ç”¨webdriver-managerä¸‹è½½ChromeDriverï¼ˆ{name}ï¼‰...")
                if mirror is None:
                    os.environ.pop('WDM_MIRROR', None)
                else:
                    os.environ['WDM_MIRROR'] = mirror
                os.environ['WDM_CACHE_PATH'] = os.path.abspath('./chromedriver_cache')
                os.environ['WDM_LOCAL'] = '0'
                os.environ['WDM_SSL_VERIFY'] = 'false'
                chromedriver_path = ChromeDriverManager().install()
                print(f"âœ… webdriver-managerï¼ˆ{name}ï¼‰ä¸‹è½½æˆåŠŸ: {chromedriver_path}")
                return chromedriver_path
            except Exception as e:
                print(f"âŒ webdriver-managerï¼ˆ{name}ï¼‰ä¸‹è½½å¤±è´¥: {e}")

    # æ–¹æ¡ˆ3: æ‰‹åŠ¨ä¸‹è½½
    print("ğŸ”§ å°è¯•æ‰‹åŠ¨ä¸‹è½½ChromeDriver...")
    chromedriver_path = download_chromedriver_manual()
    if chromedriver_path:
        print(f"âœ… æ‰‹åŠ¨ä¸‹è½½æˆåŠŸ: {chromedriver_path}")
        return chromedriver_path

    # æ–¹æ¡ˆ4: æç¤ºç”¨æˆ·æ‰‹åŠ¨å®‰è£…
    print("âŒ æ— æ³•è‡ªåŠ¨è·å–ChromeDriverï¼ˆå·²å°è¯•è‡ªåŠ¨æ£€æµ‹ç‰ˆæœ¬ã€é»˜è®¤æºã€é˜¿é‡Œäº‘ã€æ¸…åæºå’Œæ‰‹åŠ¨ä¸‹è½½ï¼‰")
    print("è¯·æ‰‹åŠ¨ä¸‹è½½ChromeDriverå¹¶æ”¾ç½®åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€:")
    if platform.system() == "Windows":
        print("- å½“å‰ç›®å½•ä¸‹çš„chromedriver.exe")
        print("- C:\\chromedriver\\chromedriver.exe")
        print("- æ·»åŠ åˆ°ç³»ç»ŸPATHç¯å¢ƒå˜é‡")
    else:
        print("- å½“å‰ç›®å½•ä¸‹çš„chromedriver")
        print("- /usr/local/bin/chromedriver")
        print("- æ·»åŠ åˆ°ç³»ç»ŸPATHç¯å¢ƒå˜é‡")
    return None


def load_titles():
    if not os.path.exists(TXT_FILE):
        return set()
    with open(TXT_FILE, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f if line.strip())


def save_title(title):
    with open(TXT_FILE, 'a', encoding='utf-8') as f:
        f.write(title + '\n')


def safe_publish_time(publish_time):
    today = datetime.now()

    if not publish_time:
        return 'unknown'

    # åŒ¹é…ä¿„è¯­æ—¶é—´æ ¼å¼ "5 Ğ¸ÑĞ»Ñ 2025"
    m = re.search(r'(\d{1,2})\s+([Ğ°-ÑÑ‘]+)\s+(\d{4})', publish_time)
    if m:
        day = m.group(1).zfill(2)
        month_ru = m.group(2).lower()
        month = MONTH_MAP.get(month_ru, '01')
        year = m.group(3)
        return f'{year}-{month}-{day}'

    # åŒ¹é… "ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ, 15:30" æ ¼å¼
    m = re.search(r'ÑĞµĞ³Ğ¾Ğ´Ğ½Ñ, (\d{1,2}):(\d{2})', publish_time)
    if m:
        return today.strftime('%Y-%m-%d')

    # åŒ¹é… "Ğ²Ñ‡ĞµÑ€Ğ°, 15:30" æ ¼å¼
    m = re.search(r'Ğ²Ñ‡ĞµÑ€Ğ°, (\d{1,2}):(\d{2})', publish_time)
    if m:
        dt = today - timedelta(days=1)
        return dt.strftime('%Y-%m-%d')

    # åŒ¹é… "2025-07-05" æ ¼å¼
    m = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', publish_time)
    if m:
        year = m.group(1)
        month = m.group(2).zfill(2)
        day = m.group(3).zfill(2)
        return f'{year}-{month}-{day}'

    # æ–°å¢ï¼šåŒ¹é…07.07.2025 16:00æ ¼å¼
    m = re.search(r'(\d{2})\.(\d{2})\.(\d{4})\s+(\d{2}):(\d{2})', publish_time)
    if m:
        day = m.group(1)
        month = m.group(2)
        year = m.group(3)
        hour = m.group(4)
        minute = m.group(5)
        return f'{year}-{month}-{day} {hour}:{minute}:00'

    # æ–°å¢ï¼šåŒ¹é…07.07.2025æ ¼å¼ï¼ˆåªæœ‰æ—¥æœŸï¼‰
    m = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', publish_time)
    if m:
        day = m.group(1)
        month = m.group(2)
        year = m.group(3)
        return f'{year}-{month}-{day}'

    return 'unknown'


def safe_filename(s):
    return re.sub(r'[^\w\u4e00-\u9fa5]', '', s)


def get_latest_date_from_titles():
    """ä»æ ‡é¢˜æ–‡ä»¶ä¸­æå–æœ€æ–°æ—¥æœŸ"""
    if not os.path.exists(TXT_FILE):
        return None

    latest_date = None
    with open(TXT_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            # å°è¯•ä»æ ‡é¢˜è¡Œä¸­æå–æ—¥æœŸä¿¡æ¯
            match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', line)
            if match:
                day = match.group(1)
                month = match.group(2)
                year = match.group(3)
                try:
                    date_obj = datetime(int(year), int(month), int(day))
                    if latest_date is None or date_obj > latest_date:
                        latest_date = date_obj
                except:
                    continue

    return latest_date


def get_previous_day_date():
    """è·å–å‰ä¸€å¤©çš„æ—¥æœŸï¼ˆæ ¼å¼ä¸ºYYMMDDï¼‰"""
    # å°è¯•ä»æ ‡é¢˜æ–‡ä»¶ä¸­è·å–æœ€æ–°æ—¥æœŸ
    latest_date = get_latest_date_from_titles()
    if latest_date:
        # å‡å»ä¸€å¤©
        prev_day = latest_date - timedelta(days=1)
        # æ ¼å¼åŒ–ä¸ºYYMMDD
        return prev_day.strftime('%y%m%d')

    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸå‰ä¸€å¤©
    prev_day = datetime.now() - timedelta(days=1)
    return prev_day.strftime('%y%m%d')


def save_articles_grouped_by_date(articles, channel_name):
    from collections import defaultdict
    grouped = defaultdict(list)
    for art in articles:
        # åªå–å¹´æœˆæ—¥éƒ¨åˆ†ä½œä¸ºåˆ†ç»„ä¾æ®
        date_str = art['metadata']['publish_time'][:10] if art['metadata']['publish_time'] else 'unknown'
        grouped[date_str].append(art)

    now_str = datetime.now().strftime('%H%M%S')
    cat = safe_filename(channel_name)

    for date_str, arts in grouped.items():
        # å¤„ç†æ—¶é—´å­—ç¬¦ä¸²ï¼Œç¡®ä¿æ–‡ä»¶åå®‰å…¨
        if ' ' in date_str:  # å¦‚æœåŒ…å«æ—¶é—´éƒ¨åˆ†ï¼Œåªå–æ—¥æœŸéƒ¨åˆ†
            date_str = date_str.split(' ')[0]

        # ä½¿ç”¨åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
        original_date_str = date_str

        # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        pt = date_str.replace('-', '')
        # å°†4ä½å¹´ä»½ç¼©çŸ­ä¸º2ä½å¹´ä»½
        if len(pt) == 8 and pt.isdigit():  # ç¡®ä¿æ˜¯8ä½æ•°å­—æ ¼å¼
            pt = pt[2:]  # å»æ‰å‰ä¸¤ä½å¹´ä»½ï¼Œåªä¿ç•™åä¸¤ä½

        # å¦‚æœæ˜¯unknownï¼Œä½¿ç”¨å‰ä¸€å¤©æ—¥æœŸ
        if pt == 'unknown':
            prev_day = get_previous_day_date()
            print(f"âš ï¸ ä½¿ç”¨å‰ä¸€å¤©æ—¥æœŸæ›¿ä»£unknown: {prev_day}")
            pt = prev_day

        # å¦‚æœè½¬æ¢åæ—¥æœŸæ— æ•ˆï¼ˆä¸æ˜¯6ä½æ•°å­—ï¼‰ï¼Œä½¿ç”¨å‰ä¸€å¤©æ—¥æœŸ
        if len(pt) != 6 or not pt.isdigit():
            prev_day = get_previous_day_date()
            print(f"âš ï¸ æ— æ•ˆæ—¥æœŸ '{pt}'ï¼Œä½¿ç”¨å‰ä¸€å¤©æ—¥æœŸæ›¿ä»£: {prev_day}")
            pt = prev_day

        filename = f'146_{cat}_{pt}_{now_str}.json'
        filepath = os.path.join(JSON_DIR, filename)

        # ä¿®æ­£ï¼šå…¨éƒ¨è¦†ç›–categoryå­—æ®µä¸ºcat
        for art in arts:
            if 'metadata' in art:
                art['metadata']['category'] = cat

        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(arts, f, ensure_ascii=False, indent=2)
            print(f'ğŸ’¾ å·²ä¿å­˜{len(arts)}ç¯‡æ–‡ç« åˆ° {filepath} (åŸå§‹æ—¥æœŸ: {original_date_str})')
        except Exception as e:
            print(f'âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {filepath}, é”™è¯¯: {str(e)}')
            # å°è¯•ä½¿ç”¨å¤‡ç”¨æ–‡ä»¶å
            backup_filename = f'146_{cat}_backup_{now_str}.json'
            backup_filepath = os.path.join(JSON_DIR, backup_filename)
            try:
                with open(backup_filepath, 'w', encoding='utf-8') as f:
                    json.dump(arts, f, ensure_ascii=False, indent=2)
                print(f'ğŸ’¾ å·²ä¿å­˜{len(arts)}ç¯‡æ–‡ç« åˆ°å¤‡ç”¨æ–‡ä»¶ {backup_filepath}')
            except Exception as e2:
                print(f'âŒ å¤‡ç”¨æ–‡ä»¶ä¿å­˜ä¹Ÿå¤±è´¥: {str(e2)}')


def crawl_article(url, session=None):
    if session is None:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })

    for attempt in range(3):
        try:
            # å¢åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬è™«æ£€æµ‹
            sleep(1 + attempt * 0.5)

            # ä½¿ç”¨æ›´é•¿çš„è¶…æ—¶æ—¶é—´
            response = session.get(url, timeout=30, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # æŸ¥æ‰¾æ ‡é¢˜ - æ ¹æ®æä¾›çš„HTMLç»“æ„
            title_elem = soup.find('h1', class_='PageArticleCommonTitle_title__fUDQW')
            if not isinstance(title_elem, Tag):
                print(f"  Ã— æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡")
                if attempt < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                    continue
                else:
                    print(f"  Ã— è¿ç»­3æ¬¡æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ï¼Œè·³è¿‡: {url}")
                    return None, None, None

            title_text = title_elem.get_text(strip=True)

            # ä¼˜å…ˆæŸ¥æ‰¾ä½œè€… - PageArticleContent_authors__eRDtn ä¸‹æ‰€æœ‰ <a> æ ‡ç­¾æ–‡æœ¬
            authors = ''
            author_elem = soup.find(class_='PageArticleContent_authors__eRDtn')
            if isinstance(author_elem, Tag):
                a_tags = author_elem.find_all('a')
                authors = ' '.join(a.get_text(strip=True) for a in a_tags if a.get_text(strip=True))
            else:
                # å…¶æ¬¡æŸ¥æ‰¾ PageArticle_authors__cFIb5 ä¸‹æ‰€æœ‰ <a> æ ‡ç­¾æ–‡æœ¬
                author_elem = soup.find(class_='PageArticle_authors__cFIb5')
                if isinstance(author_elem, Tag):
                    a_tags = author_elem.find_all('a')
                    authors = ' '.join(a.get_text(strip=True) for a in a_tags if a.get_text(strip=True))
                else:
                    # å¤‡é€‰ä½œè€…æŸ¥æ‰¾é€»è¾‘
                    author_elem = (
                            soup.find('span', class_='author') or
                            soup.find('div', class_='author') or
                            soup.find('span', class_='byline') or
                            soup.find('div', class_='byline')
                    )
                    if isinstance(author_elem, Tag):
                        authors = author_elem.get_text(strip=True)

            # æŸ¥æ‰¾æ–‡ç« å†…å®¹ - æ ¹æ®æä¾›çš„HTMLç»“æ„
            content_elem = soup.find('div', class_='PageContentCommonStyling_text__CKOzO')
            if not isinstance(content_elem, Tag):
                print(f"  Ã— æœªæ‰¾åˆ°å†…å®¹å…ƒç´ ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡")
                if attempt < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                    continue
                else:
                    print(f"  Ã— è¿ç»­3æ¬¡æœªæ‰¾åˆ°å†…å®¹å…ƒç´ ï¼Œè·³è¿‡: {url}")
                    return None, None, None

            # æå–æ®µè½å†…å®¹ - æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾ï¼Œç‰¹åˆ«æ˜¯å¸¦æœ‰_msttexthashå±æ€§çš„
            paragraphs = []
            # æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾
            all_p_tags = content_elem.find_all('p')
            for p in all_p_tags:
                if isinstance(p, Tag):
                    # æ£€æŸ¥æ˜¯å¦æœ‰_msttexthashå±æ€§
                    class_attr = p.get('class')
                    has_msttexthash = class_attr and any('_msttexthash' in str(cls) for cls in class_attr)
                    text = p.get_text(strip=True)
                    if text and len(text) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                        paragraphs.append(text)
                        if has_msttexthash:
                            print(f"    âœ“ æ‰¾åˆ°å¸¦_msttexthashå±æ€§çš„æ®µè½: {text[:50]}...")

            # å¦‚æœä¸Šé¢çš„æ–¹æ³•æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            if not paragraphs:
                # æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾ï¼Œä¸ç®¡æ˜¯å¦æœ‰_msttexthashå±æ€§
                all_p_tags = content_elem.find_all('p')
                for p in all_p_tags:
                    if isinstance(p, Tag):
                        text = p.get_text(strip=True)
                        if text and len(text) > 10:
                            paragraphs.append(text)

            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œé‡è¯•
            if not paragraphs:
                print(f"  Ã— æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡")
                if attempt < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                    continue
                else:
                    print(f"  Ã— è¿ç»­3æ¬¡æœªæ‰¾åˆ°æ–‡ç« å†…å®¹ï¼Œè·³è¿‡: {url}")
                    return None, None, None

            content = ''.join(paragraphs)

            # æŸ¥æ‰¾å‘å¸ƒæ—¶é—´ï¼ˆåªç”¨ContentMetaDefault_date__wS0teç±»ï¼‰
            publish_time = ''
            time_elem = soup.find(class_='ContentMetaDefault_date__wS0te')
            if isinstance(time_elem, Tag):
                publish_time = time_elem.get_text(strip=True)

            # ç¡®å®šåˆ†ç±»
            category = "æ–°é—»"
            if '/tema/gos' in url:
                category = "æ”¿åºœ"
            elif '/tema/ekonomika' in url:
                category = "ç»æµ"
            elif '/tema/mir' in url:
                category = "å›½é™…"
            elif '/tema/obshestvo' in url:
                category = "ç¤¾ä¼š"
            elif '/tema/bezopasnost' in url:
                category = "å®‰å…¨"

            article_data = {
                "title": title_text,
                "content": content,
                "sources": {
                    "current_site": "ä¿„ç½—æ–¯æŠ¥",
                    "current_siteurl": "rg.ru",
                    "origin_url": url
                },
                "metadata": {
                    "publish_time": safe_publish_time(publish_time),
                    "authors": authors,
                    "category": category
                },
                "crawlingtime": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            return article_data, title_text, publish_time

        except requests.exceptions.SSLError as e:
            print(f"  Ã— SSLé”™è¯¯ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡: {url}")
            sleep(3 + attempt * 2)  # å¢åŠ å»¶è¿Ÿæ—¶é—´
            continue
        except requests.exceptions.ConnectionError as e:
            print(f"  Ã— è¿æ¥é”™è¯¯ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡: {url}")
            sleep(3 + attempt * 2)
            continue
        except requests.exceptions.Timeout as e:
            print(f"  Ã— è¶…æ—¶é”™è¯¯ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡: {url}")
            sleep(3 + attempt * 2)
            continue
        except Exception as e:
            print(f"  Ã— çˆ¬å–æ–‡ç« å¤±è´¥ {url}: {str(e)}")
            sleep(2)
            continue

    return None, None, None


def extract_article_links_from_page_rubric(soup, base_url):
    """ä»PageRubricSeo_text__9XF1Jå…ƒç´ ä¸­æå–æ–‡ç« é“¾æ¥"""
    urls = []

    # æŸ¥æ‰¾PageRubricSeo_text__9XF1Jå…ƒç´ 
    page_rubric_elements = soup.find_all('div', class_='PageRubricSeo_text__9XF1J')

    for element in page_rubric_elements:
        # åœ¨PageRubricSeo_text__9XF1Jå…ƒç´ å†…æŸ¥æ‰¾æ–‡ç« é“¾æ¥
        links = element.find_all('a', href=True)
        for link in links:
            if isinstance(link, Tag):
                href = link.get('href')
                if isinstance(href, str):
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if href.startswith('/'):
                        full_url = 'https://rg.ru' + href
                    elif href.startswith('https://rg.ru/'):
                        full_url = href
                    else:
                        continue

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
                    if '/202' in full_url and '.html' in full_url:
                        urls.append(full_url)

    # åŒæ—¶æŸ¥æ‰¾ItemOfListStandard_title__Ajjlfç±»çš„é“¾æ¥
    title_links = soup.find_all('span', class_='ItemOfListStandard_title__Ajjlf')
    for title_span in title_links:
        if isinstance(title_span, Tag):
            parent_link = title_span.find_parent('a')
            if isinstance(parent_link, Tag):
                href = parent_link.get('href')
                if isinstance(href, str):
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if href.startswith('/'):
                        full_url = 'https://rg.ru' + href
                    elif href.startswith('https://rg.ru/'):
                        full_url = href
                    else:
                        continue

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
                    if '/202' in full_url and '.html' in full_url:
                        urls.append(full_url)

    return list(set(urls))


def extract_article_links_from_page(soup, base_url):
    """ä»é¡µé¢ä¸­æå–æ–‡ç« é“¾æ¥"""
    urls = []

    # æŸ¥æ‰¾ItemOfListStandard_title__Ajjlfç±»çš„é“¾æ¥
    title_links = soup.find_all('span', class_='ItemOfListStandard_title__Ajjlf')
    for title_span in title_links:
        if isinstance(title_span, Tag):
            parent_link = title_span.find_parent('a')
            if isinstance(parent_link, Tag):
                href = parent_link.get('href')
                if isinstance(href, str):
                    # å¤„ç†ç›¸å¯¹é“¾æ¥
                    if href.startswith('/'):
                        full_url = 'https://rg.ru' + href
                    elif href.startswith('https://rg.ru/'):
                        full_url = href
                    else:
                        continue

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
                    if '/202' in full_url and '.html' in full_url:
                        urls.append(full_url)

    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é“¾æ¥
    links = soup.find_all('a', href=True)
    for link in links:
        if isinstance(link, Tag):
            href = link.get('href')
            if isinstance(href, str):
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if href.startswith('/'):
                    full_url = 'https://rg.ru' + href
                elif href.startswith('https://rg.ru/'):
                    full_url = href
                else:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯æ–‡ç« é“¾æ¥
                if '/202' in full_url and '.html' in full_url:
                    urls.append(full_url)

    return list(set(urls))


def crawl_channel(channel_url, driver=None, unique_temp_dir=None, chromedriver_path=None):
    print(f"\nğŸŒ åŠ è½½é¢‘é“: {channel_url}")
    # åªåœ¨driverä¸ºNoneæ—¶æ‰åˆ›å»ºæ–°å®ä¾‹ï¼Œå¦åˆ™å§‹ç»ˆå¤ç”¨
    if driver is None:
        print("ğŸ”§ åˆ›å»ºæ–°çš„æµè§ˆå™¨å®ä¾‹...")
        import uuid
        unique_temp_dir = os.path.abspath(f'./chrome_temp_{uuid.uuid4().hex[:8]}')
        os.makedirs(unique_temp_dir, exist_ok=True)
        chrome_options = Options()
        chrome_options.add_argument('--headless=new')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument(f'--user-data-dir={unique_temp_dir}')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        try:
            if chromedriver_path is None:
                chromedriver_path = get_chromedriver_path()
                if chromedriver_path is None:
                    print("âŒ æ— æ³•è·å–ChromeDriverï¼Œè·³è¿‡å½“å‰é¢‘é“")
                    return None, None
            service = Service(chromedriver_path)
            driver = webdriver.Chrome(options=chrome_options, service=service)
        except Exception as e:
            print(f"ChromeDriverå¯åŠ¨å¤±è´¥: {e}")
            if unique_temp_dir and os.path.exists(unique_temp_dir):
                try:
                    shutil.rmtree(unique_temp_dir)
                except:
                    pass
            return None, None
    else:
        print("â™»ï¸ å¤ç”¨ç°æœ‰æµè§ˆå™¨å®ä¾‹...")
        # å¤ç”¨æ—¶ä¸å†æ›´æ¢unique_temp_dir
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    try:
        driver.get(channel_url)
    except Exception as e:
        print(f'âš ï¸ è®¿é—®é¢‘é“å¤±è´¥: {e}')
        return driver, unique_temp_dir

    sleep(3)

    max_scrolls = 50  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°
    scroll_count = 0
    seen_links = set()
    titles_set = load_titles()
    print(f"å·²åŠ è½½ {len(titles_set)} ä¸ªå†å²æ ‡é¢˜ç”¨äºå»é‡")

    # æ ¹æ®URLç¡®å®šé¢‘é“åç§°
    if '/tema/gos' in channel_url:
        channel_name = 'æ”¿åºœ'
    elif '/tema/ekonomika' in channel_url:
        channel_name = 'ç»æµ'
    elif '/tema/mir' in channel_url:
        channel_name = 'å›½é™…'
    elif '/tema/obshestvo' in channel_url:
        channel_name = 'ç¤¾ä¼š'
    elif '/tema/bezopasnost' in channel_url:
        channel_name = 'å®‰å…¨'
    else:
        channel_name = 'æ–°é—»'

    all_articles = []
    no_new_content_count = 0
    no_new_content_threshold = 5

    # åˆ›å»ºä¼šè¯ï¼Œæé«˜è¿æ¥æ•ˆç‡
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    })

    try:
        while scroll_count < max_scrolls:
            print(f"\n--- ç¬¬ {scroll_count + 1} æ¬¡æ»šåŠ¨ ---")

            # æ–°å¢ï¼šé¢‘é“æ— æ–‡ç« æ—¶é‡è¯•æœºåˆ¶
            retry_channel_count = 0
            while retry_channel_count < 3:
                html = driver.page_source
                soup = BeautifulSoup(html, 'html.parser')
                urls = extract_article_links_from_page(soup, channel_url)
                if urls:
                    break
                else:
                    retry_channel_count += 1
                    print(f"âš ï¸ æœªå‘ç°ä»»ä½•æ–‡ç« é“¾æ¥ï¼Œæ­£åœ¨é‡æ–°è¿›å…¥é¢‘é“ï¼ˆç¬¬{retry_channel_count}æ¬¡ï¼‰: {channel_url}")
                    driver.get(channel_url)
                    sleep(3 + random.uniform(1, 2))
            else:
                print(f"âŒ è¿ç»­3æ¬¡æœªèƒ½åœ¨é¢‘é“é¡µé¢å‘ç°æ–‡ç« ï¼Œè·³è¿‡è¯¥é¢‘é“: {channel_url}")
                return driver, unique_temp_dir

            new_urls = [u for u in urls if u not in seen_links]
            print(f'æœ¬è½®æ–°å‘ç° {len(new_urls)} ä¸ªé“¾æ¥')

            # æ‰¹é‡çˆ¬å–æœ¬è½®æ‰€æœ‰æ–°é“¾æ¥
            articles_this_round = []
            success_count = 0
            fail_count = 0

            try:
                for i, url in enumerate(new_urls):
                    seen_links.add(url)
                    print(f'  [{i + 1}/{len(new_urls)}] çˆ¬å–: {url}')

                    article_data, title_text, publish_time = crawl_article(url, session)
                    if not article_data or not title_text:
                        fail_count += 1
                        continue
                    if title_text in titles_set:
                        print(f'  Ã— å·²çˆ¬å–è¿‡: {title_text}')
                        continue

                    articles_this_round.append(article_data)
                    save_title(title_text)
                    titles_set.add(title_text)
                    success_count += 1
                    print(f'  âœ… æ–°æ–‡ç« : {title_text}')

                    # å¢åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«åçˆ¬è™«æ£€æµ‹
                    sleep(2 + (i % 3) * 0.5)
            except KeyboardInterrupt:
                print(f"\nâš ï¸ åœ¨çˆ¬å–è¿‡ç¨‹ä¸­æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜å½“å‰è½®å·²çˆ¬å–çš„{len(articles_this_round)}ç¯‡æ–‡ç« ...")
                if articles_this_round:
                    if not os.path.exists(JSON_DIR):
                        os.makedirs(JSON_DIR)
                    save_articles_grouped_by_date(articles_this_round, channel_name)
                    all_articles.extend(articles_this_round)
                    print(f"âœ… å·²ä¿å­˜å½“å‰è½®{len(articles_this_round)}ç¯‡æ–‡ç« ")
                raise

            print(f'  æœ¬è½®æˆåŠŸ: {success_count}, å¤±è´¥: {fail_count}')

            # å¦‚æœå¤±è´¥ç‡è¿‡é«˜ï¼Œæš‚åœä¸€æ®µæ—¶é—´
            if len(new_urls) > 0 and fail_count / len(new_urls) > 0.7:
                print(f"âš ï¸ å¤±è´¥ç‡è¿‡é«˜ ({fail_count}/{len(new_urls)})ï¼Œæš‚åœ30ç§’...")
                sleep(30)

            if not os.path.exists(JSON_DIR):
                os.makedirs(JSON_DIR)
            if articles_this_round:
                save_articles_grouped_by_date(articles_this_round, channel_name)
                all_articles.extend(articles_this_round)
                no_new_content_count = 0
                print(f"âœ… æœ¬è½®å·²ä¿å­˜{len(articles_this_round)}ç¯‡æ–‡ç« åˆ°JSONæ–‡ä»¶")
            else:
                no_new_content_count += 1
                print(f"æœ¬è½®æœªå‘ç°æ–°å†…å®¹ï¼Œç´¯è®¡{no_new_content_count}æ¬¡")
                if no_new_content_count >= no_new_content_threshold:
                    print(f"è¿ç»­{no_new_content_threshold}æ¬¡æœªå‘ç°æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨")
                    break

            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ï¼Œç­‰å¾…PageRubricSeo_text__9XF1Jå…ƒç´ åŠ è½½
            try:
                last_height = driver.execute_script("return document.body.scrollHeight")
                # å¹³æ»‘ä¸‹æ»‘åˆ°åº•éƒ¨
                for y in range(0, last_height, 200):
                    driver.execute_script(f"window.scrollTo(0, {y});")
                    sleep(0.02)
                driver.execute_script(f"window.scrollTo(0, {last_height});")
                sleep(0.2)
                # å†å¾€ä¸Šæ»‘ä¸€å°éƒ¨åˆ†ï¼Œæ¨¡æ‹ŸçœŸäººæ“ä½œ
                driver.execute_script(f"window.scrollTo(0, {last_height - 300});")
                sleep(0.2)
                print("å·²å¹³æ»‘æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨å¹¶ä¸Šæ»‘ä¸€å°æ®µï¼Œç­‰å¾…PageRubricSeo_wrapper__gIhVVå…ƒç´ åŠ è½½")
                sleep(5 + random.uniform(1, 3))  # å¢åŠ éšæœºå»¶è¿Ÿ

                # ç­‰å¾…PageRubricSeo_wrapper__gIhVVå…ƒç´ å‡ºç°
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "PageRubricSeo_wrapper__gIhVV"))
                    )
                    print("PageRubricSeo_wrapper__gIhVVå…ƒç´ å·²åŠ è½½")
                except:
                    print("æœªæ£€æµ‹åˆ°PageRubricSeo_wrapper__gIhVVå…ƒç´ ï¼Œç»§ç»­å°è¯•...")

                # ç­‰å¾…PageRubricSeo_text__9XF1Jå…ƒç´ å‡ºç°
                try:
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "PageRubricSeo_text__9XF1J"))
                    )
                    print("PageRubricSeo_text__9XF1Jå…ƒç´ å·²åŠ è½½")
                    sleep(5)  # å…ƒç´ åŠ è½½åç­‰å¾…5ç§’
                except:
                    print("æœªæ£€æµ‹åˆ°PageRubricSeo_text__9XF1Jå…ƒç´ ï¼Œç»§ç»­å°è¯•...")

                # æ–°å¢ï¼šç­‰å¾…5ç§’åæ£€æŸ¥é¡µé¢é«˜åº¦å˜åŒ–ï¼Œå¦åˆ™å°è¯•ç‚¹å‡»LoadMoreæŒ‰é’®
                loadmore_fail_count = 0
                while True:
                    new_height = driver.execute_script("return document.body.scrollHeight")
                    if new_height != last_height:
                        print(f"é¡µé¢é«˜åº¦ä» {last_height} å¢åŠ åˆ° {new_height}")
                        no_new_content_count = 0
                        break
                    else:
                        print("é¡µé¢é«˜åº¦æœªå˜åŒ–ï¼Œå°è¯•ç‚¹å‡»LoadMoreæŒ‰é’®...")
                        # åªè¦æ˜¯class="LoadMoreBtn_wrapper__A7ItH   "çš„buttonéƒ½ç‚¹å‡»
                        btns = driver.find_elements(By.XPATH,
                                                    '//button[contains(@class, "LoadMoreBtn_wrapper__A7ItH")]')
                        found = False
                        for btn in btns:
                            try:
                                btn.click()
                                print("å·²ç‚¹å‡»LoadMoreæŒ‰é’®ï¼Œç­‰å¾…å†…å®¹åŠ è½½...")
                                sleep(3 + random.uniform(1, 2))
                                found = True
                                break
                            except Exception as e:
                                print(f"ç‚¹å‡»LoadMoreæŒ‰é’®å¼‚å¸¸: {e}")
                        if not found:
                            print("æœªæ‰¾åˆ°LoadMoreæŒ‰é’®ï¼Œç­‰å¾…åé‡è¯•...")
                            sleep(3 + random.uniform(1, 2))
                        loadmore_fail_count += 1
                        if loadmore_fail_count >= 5:
                            print("è¿ç»­5æ¬¡ç­‰å¾…å’Œç‚¹å‡»LoadMoreéƒ½æ— æ•ˆï¼Œè·³åˆ°ä¸‹ä¸€ä¸ªé¢‘é“")
                            break
                scroll_count += 1

            except Exception as e:
                print(f"æ»šåŠ¨å¤±è´¥: {str(e)}")
                no_new_content_count += 1
                if no_new_content_count >= no_new_content_threshold:
                    print(f"è¿ç»­{no_new_content_threshold}æ¬¡æ»šåŠ¨å¤±è´¥ï¼Œåœæ­¢")
                    break
                sleep(3 + random.uniform(1, 2))
                continue

    except KeyboardInterrupt:
        print("\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰ï¼Œæ­£åœ¨ä¿å­˜å·²çˆ¬å–å†…å®¹...")
        print(f"ğŸ“Š å½“å‰é¢‘é“ç»Ÿè®¡ï¼š")
        print(f"  - all_articlesåˆ—è¡¨é•¿åº¦: {len(all_articles)}")
        print(f"  - å·²è§è¿‡çš„é“¾æ¥æ•°: {len(seen_links)}")
        print(f"  - æ»šåŠ¨æ¬¡æ•°: {scroll_count}")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„æ–‡ç« 
        if all_articles:
            print(f"\nâš ï¸ æ­£åœ¨ä¿å­˜å·²çˆ¬å–çš„{len(all_articles)}ç¯‡æ–‡ç« ...")
            save_articles_grouped_by_date(all_articles, channel_name)
        else:
            print("\nâœ… æ‰€æœ‰æ–‡ç« å·²åœ¨æ¯è½®ä¸­ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼Œæ— éœ€é¢å¤–ä¿å­˜")
        raise
    except Exception as e:
        print(f"\nâŒ é¢‘é“çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        traceback.print_exc()
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„æ–‡ç« 
        if all_articles:
            print(f"\nâš ï¸ å°è¯•ä¿å­˜å·²çˆ¬å–çš„{len(all_articles)}ç¯‡æ–‡ç« ...")
            save_articles_grouped_by_date(all_articles, channel_name)
        # è¿”å›driverå’Œunique_temp_dirä¾›åç»­ä½¿ç”¨
        return driver, unique_temp_dir
    finally:
        print(f"\nğŸ“Š é¢‘é“çˆ¬å–å®Œæˆç»Ÿè®¡:")
        print(f"  - æ€»æ–‡ç« æ•°: {len(all_articles)}")
        print(f"  - å·²è§è¿‡çš„é“¾æ¥æ•°: {len(seen_links)}")
        print(f"  - æ»šåŠ¨æ¬¡æ•°: {scroll_count}")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„æ–‡ç« 
        if all_articles:
            print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜å·²çˆ¬å–çš„{len(all_articles)}ç¯‡æ–‡ç« ...")
            save_articles_grouped_by_date(all_articles, channel_name)
        else:
            print("\nâœ… æ‰€æœ‰æ–‡ç« å·²åœ¨æ¯è½®ä¸­ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼Œæ— éœ€é¢å¤–ä¿å­˜")
        # è¿”å›driverå’Œunique_temp_dirä¾›åç»­é¢‘é“ä½¿ç”¨
        return driver, unique_temp_dir


def run_crawler():
    print("ğŸ¯ RG.ru é¢‘é“é€æ­¥çˆ¬è™«å¯åŠ¨")

    # æŒ‡å®šçš„é¢‘é“åˆ—è¡¨
    channels = [
        "https://rg.ru/tema/gos",  # æ”¿åºœ
        "https://rg.ru/tema/ekonomika",  # ç»æµ
        "https://rg.ru/tema/mir",  # å›½é™…
        "https://rg.ru/tema/obshestvo",  # ç¤¾ä¼š
        "https://rg.ru/tema/bezopasnost"  # å®‰å…¨
    ]

    chromedriver_path = get_chromedriver_path()
    if chromedriver_path is None:
        print("âŒ æ— æ³•è·å–ChromeDriverï¼Œç¨‹åºé€€å‡º")
        return

    driver = None
    unique_temp_dir = None
    try:
        for i, channel_url in enumerate(channels):
            try:
                print(f"\nğŸ“º å¼€å§‹çˆ¬å–ç¬¬{i + 1}ä¸ªé¢‘é“: {channel_url}")
                driver, unique_temp_dir = crawl_channel(channel_url, driver, unique_temp_dir, chromedriver_path)
                if driver is None:
                    print("âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡åç»­é¢‘é“")
                    break
            except Exception as e:
                print(f"âŒ çˆ¬å–é¢‘é“ {channel_url} æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                traceback.print_exc()
                # ç»§ç»­ä¸‹ä¸€ä¸ªé¢‘é“
                continue
        print("\nğŸ¯ æ‰€æœ‰é¢‘é“çˆ¬å–å®Œæˆï¼")
    except KeyboardInterrupt:
        print("\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰ï¼Œæ­£åœ¨æ¸…ç†èµ„æº...")
        if driver:
            try:
                driver.quit()
                print("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            except:
                pass
        if unique_temp_dir and os.path.exists(unique_temp_dir):
            try:
                shutil.rmtree(unique_temp_dir)
                print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•")
            except:
                pass
        return
    except Exception as e:
        print(f"âŒ çˆ¬è™«è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        traceback.print_exc()
    finally:
        # ç¡®ä¿æµè§ˆå™¨è¢«å…³é—­ï¼ˆåªåœ¨æ‰€æœ‰é¢‘é“åå…³é—­ï¼‰
        if driver:
            try:
                driver.quit()
                print("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            except:
                pass
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if unique_temp_dir and os.path.exists(unique_temp_dir):
            try:
                shutil.rmtree(unique_temp_dir)
                print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•")
            except:
                pass
        # æ¸…ç†ChromeDriverç¼“å­˜
        try:
            if os.path.exists('./chromedriver_cache'):
                shutil.rmtree('./chromedriver_cache')
                print("ğŸ§¹ å·²æ¸…ç†ChromeDriverç¼“å­˜ç›®å½•")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ChromeDriverç¼“å­˜å¤±è´¥: {e}")


def calculate_next_run():
    """è®¡ç®—ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶é—´ï¼ˆç¬¬äºŒå¤©æ—©ä¸Š6ç‚¹ï¼‰"""
    now = datetime.now()
    next_run = now.replace(hour=6, minute=0, second=0, microsecond=0)
    if now.hour >= 6:
        next_run += timedelta(days=1)
    return next_run


def main():
    global exception_count

    # é¦–æ¬¡è¿è¡Œè®¡æ•°å™¨
    first_run = True

    while True:
        try:
            if first_run:
                print("ğŸš€ é¦–æ¬¡è¿è¡Œï¼šç«‹å³å¯åŠ¨çˆ¬è™«")
                run_crawler()
                first_run = False
                # é‡ç½®å¼‚å¸¸è®¡æ•°å™¨
                exception_count = 0
            else:
                # è®¡ç®—ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶é—´
                next_run_time = calculate_next_run()
                wait_seconds = (next_run_time - datetime.now()).total_seconds()

                if wait_seconds > 0:
                    print(f"\nâ³ çˆ¬è™«å®Œæˆï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡è¿è¡Œæ—¶é—´: {next_run_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    print(f"  å‰©ä½™ç­‰å¾…æ—¶é—´: {wait_seconds:.0f}ç§’ ({wait_seconds / 3600:.1f}å°æ—¶)")

                    # æ¯å°æ—¶æŠ¥å‘Šä¸€æ¬¡çŠ¶æ€
                    while wait_seconds > 0:
                        hours = wait_seconds // 3600
                        minutes = (wait_seconds % 3600) // 60
                        seconds = wait_seconds % 60

                        if hours > 0:
                            print(f"  ä¼‘çœ å€’è®¡æ—¶: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’", end='\r')
                        elif minutes > 0:
                            print(f"  ä¼‘çœ å€’è®¡æ—¶: {int(minutes)}åˆ†é’Ÿ {int(seconds)}ç§’", end='\r')
                        else:
                            print(f"  ä¼‘çœ å€’è®¡æ—¶: {int(seconds)}ç§’", end='\r')

                        sleep(1)
                        wait_seconds -= 1

                    print("\nâ° åˆ°è¾¾é¢„å®šæ—¶é—´ï¼Œå¯åŠ¨çˆ¬è™«")
                    run_crawler()
                    # æˆåŠŸè¿è¡Œåé‡ç½®å¼‚å¸¸è®¡æ•°å™¨
                    exception_count = 0
                else:
                    print("âš ï¸ ç­‰å¾…æ—¶é—´ä¸ºè´Ÿï¼Œç«‹å³å¯åŠ¨çˆ¬è™«")
                    run_crawler()
                    # æˆåŠŸè¿è¡Œåé‡ç½®å¼‚å¸¸è®¡æ•°å™¨
                    exception_count = 0

        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ç¨‹åºï¼Œé€€å‡º")
            break

        except Exception as e:
            exception_count += 1
            print(f"\nâŒ å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ (ç¬¬ {exception_count} æ¬¡): {str(e)}")
            traceback.print_exc()

            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¼‚å¸¸é‡è¯•æ¬¡æ•°
            if exception_count >= MAX_EXCEPTION_RETRY:
                print(f"âŒ å·²è¾¾åˆ°æœ€å¤§å¼‚å¸¸é‡è¯•æ¬¡æ•° ({MAX_EXCEPTION_RETRY})ï¼Œç¨‹åºé€€å‡º")
                break

            # è®¡ç®—å†·å´æ—¶é—´ï¼ˆéšå¼‚å¸¸æ¬¡æ•°å¢åŠ ï¼‰
            cooldown = EXCEPTION_COOLDOWN * exception_count
            print(f"ğŸ”„ {cooldown}ç§’åé‡å¯çˆ¬è™«...")

            # å¸¦å€’è®¡æ—¶çš„å†·å´ç­‰å¾…
            while cooldown > 0:
                print(f"  å†·å´å€’è®¡æ—¶: {cooldown}ç§’", end='\r')
                sleep(1)
                cooldown -= 1
            print("\nğŸ”„ å†·å´ç»“æŸï¼Œé‡å¯çˆ¬è™«")


if __name__ == '__main__':
    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    if not os.path.exists(JSON_DIR):
        os.makedirs(JSON_DIR)

    # è¿è¡Œä¸»ç¨‹åº
    main()