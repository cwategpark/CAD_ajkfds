#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Straits Times çˆ¬è™« - æœ¬åœ°æ–°é—» / å›½é™…æ–°é—» / ç»æµ
æ¯æ—¥æ—©æ™¨6ç‚¹è¿è¡Œï¼Œå¼‚å¸¸è‡ªåŠ¨é‡å¯
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import shutil
import traceback
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

driver = None

# ========== Chrome å†…æ ¸ ==========
def kernel_chrome():
    global driver
    if os.path.exists("254_chromedriver.exe"):
        print("âœ… ChromeDriverå·²å­˜åœ¨")
    else:
        driver_path = ChromeDriverManager().install()
        shutil.move(driver_path, "254_chromedriver.exe")
    target_path = os.path.join(os.getcwd(), "254_chromedriver.exe")
    print(f"âœ… ChromeDriverå·²å¤åˆ¶åˆ°: {target_path}")

    chrome_options = Options()
    chrome_options.add_argument("--headless=new")   # âœ… æ— å¤´æ¨¡å¼
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")

    service = ChromeService(executable_path=target_path)
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.get("https://www.straitstimes.com/singapore")

def dismiss_overlays():
    """å¤„ç†é¡µé¢å¯èƒ½å‡ºç°çš„å¼¹çª—"""
    try:
        close_btn = WebDriverWait(driver, 3).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, "button[aria-label='Close']"))
        )
        close_btn.click()
        print("âœ… å…³é—­å¼¹çª—")
    except:
        pass

# ========== å·¥å…·å‡½æ•° ==========
def wait_until_next_6am():
    now = datetime.now()
    tomorrow = now + timedelta(days=1)
    target_time = datetime(year=tomorrow.year, month=tomorrow.month, day=tomorrow.day, hour=6, minute=0, second=0)
    wait_seconds = (target_time - now).total_seconds()
    print(f"â³ ä¼‘çœ  {int(wait_seconds)} ç§’ï¼Œç­‰å¾…æ¬¡æ—¥6ç‚¹")
    time.sleep(wait_seconds)

def safe_publish_time(raw_time):
    """è§£æå‘å¸ƒæ—¶é—´"""
    try:
        if not raw_time:
            return ""
        raw_time = raw_time.replace("Published", "").strip()
        return raw_time
    except:
        return raw_time

# ========== JSON å­˜å‚¨ ==========
def save_articles_grouped_by_date(articles, channel_name):
    if not articles:
        return
    today = datetime.now().strftime("%Y%m%d_%H%M%S")

    # âœ… åˆ›å»º data æ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    save_dir = os.path.join(os.getcwd(), "data")
    os.makedirs(save_dir, exist_ok=True)

    filename = f"254_{channel_name}_{today}.json"
    filepath = os.path.join(save_dir, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ å·²ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° {filepath}")

# âœ… æ”¹åä¸º 254_titles.txt
titles_file = "254_titles.txt"

def load_titles():
    if os.path.exists(titles_file):
        with open(titles_file, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    return set()

def save_title(title):
    with open(titles_file, "a", encoding="utf-8") as f:
        f.write(title.strip() + "\n")

# ========== æ–‡ç« è§£æ ==========
def crawl_st_article(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        # æ ‡é¢˜
        title_elem = soup.find("h1")
        title_text = title_elem.get_text(strip=True) if title_elem else ""

        # å‘å¸ƒæ—¶é—´
        publish_elem = soup.find("p", class_="font-eyebrow-baseline-regular", string=lambda x: x and "Published" in x)
        publish_time = publish_elem.get_text(strip=True) if publish_elem else ""

        # ä½œè€…
        author_elem = soup.find("a", {"data-testid": "author-byline-default-byline-left"})
        authors = ""
        if author_elem:
            p = author_elem.find("p")
            if p:
                authors = p.get_text(strip=True)

        # æ­£æ–‡
        paragraphs = []
        for p in soup.find_all("p", {"data-testid": "article-paragraph-annotation-test-id"}):
            text = p.get_text(" ", strip=True)
            if text:
                paragraphs.append(text)
        content = "\n".join(paragraphs)

        # åˆ†ç±»
        category = "æ–°é—»"
        if "/singapore/" in url:
            category = "æœ¬åœ°æ–°é—»"
        elif "/world/" in url:
            category = "å›½é™…æ–°é—»"
        elif "/business/" in url:
            category = "ç»æµ"

        article_data = {
            "title": title_text,
            "content": content,
            "sources": {
                "current_site": "The Straits Times",
                "current_siteurl": "www.straitstimes.com",
                "origin_url": url
            },
            "metadata": {
                "publish_time": safe_publish_time(publish_time),
                "authors": authors,
                "category": category
            },
            "crawlingtime": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
        return article_data, title_text, publish_time
    except Exception as e:
        print(f"  Ã— çˆ¬å–å¤±è´¥ {url}: {e}")
        return None, None, None

# ========== ç¿»é¡µé€»è¾‘ ==========
def find_bottom_load_more(driver, wait_sec=5):
    try:
        btn = WebDriverWait(driver, wait_sec).until(
            EC.presence_of_element_located((By.XPATH, "//button//span[contains(text(), 'Load more')]/.."))
        )
        return btn
    except:
        return None

def crawl_channel(channel_url, channel_name):
    seen_links = set()
    titles_set = load_titles()
    all_articles = []
    fail_clicks = 0
    last_article_count = 0

    driver.get(channel_url)
    time.sleep(3)
    print(f"ğŸ“º å¼€å§‹çˆ¬å–é¢‘é“: {channel_name} {channel_url}")

    while True:
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        # âœ… headline-lg-card-test-id åŒºå—é‡Œçš„é“¾æ¥
        article_blocks = soup.find_all("div", {"data-testid": "headline-lg-card-test-id"})
        urls = []
        for block in article_blocks:
            parent_a = block.find_parent("a", {"data-testid": "custom-link"})
            if parent_a and parent_a.has_attr("href"):
                href = parent_a['href']
                if href.startswith("http"):
                    urls.append(href)
                else:
                    urls.append("https://www.straitstimes.com" + href)

        new_urls = [u for u in urls if u not in seen_links]
        print(f"å‘ç° {len(new_urls)} ä¸ªæ–°æ–‡ç« é“¾æ¥")

        articles_this_round = []
        for url in new_urls:
            seen_links.add(url)
            article_data, title_text, publish_time = crawl_st_article(url)
            if not article_data or not title_text:
                continue
            if title_text in titles_set:
                continue
            articles_this_round.append(article_data)
            save_title(title_text)
            titles_set.add(title_text)
            print(f"  âœ… æ–°æ–‡ç« : {title_text}")
            time.sleep(1)

        if articles_this_round:
            save_articles_grouped_by_date(articles_this_round, channel_name)
            all_articles.extend(articles_this_round)

        # ç¿»é¡µ
        load_btn = find_bottom_load_more(driver)
        if not load_btn:
            print("ğŸ›‘ æ²¡æœ‰æ›´å¤šæŒ‰é’®ï¼Œç»“æŸè¯¥é¢‘é“")
            break
        try:
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", load_btn)
            time.sleep(0.5)
            driver.execute_script("arguments[0].click();", load_btn)
            print("ç‚¹å‡» Load more")
            time.sleep(3)

            new_article_count = len(seen_links)
            if new_article_count <= last_article_count:
                fail_clicks += 1
                print(f"âš ï¸ é¡µé¢æ— æ–°å†…å®¹ï¼ˆè¿ç»­å¤±è´¥ {fail_clicks} æ¬¡ï¼‰")
            else:
                fail_clicks = 0
                last_article_count = new_article_count

            if fail_clicks >= 3:
                print("ğŸ›‘ è¿ç»­å¤šæ¬¡ç‚¹å‡»æ— æ•ˆï¼Œç»“æŸè¯¥é¢‘é“")
                break
        except Exception as e:
            fail_clicks += 1
            print(f"âŒ ç‚¹å‡»å¤±è´¥ï¼ˆè¿ç»­å¤±è´¥ {fail_clicks} æ¬¡ï¼‰: {e}")
            if fail_clicks >= 3:
                print("ğŸ›‘ è¿ç»­å¤šæ¬¡ç‚¹å‡»å¼‚å¸¸ï¼Œç»“æŸè¯¥é¢‘é“")
                break

    print(f"ğŸ‰ {channel_name} å®Œæˆï¼Œå…±è·å– {len(all_articles)} ç¯‡")

# ========== ä¸»å‡½æ•° ==========
def main():
    kernel_chrome()
    dismiss_overlays()
    channels = [
        ("https://www.straitstimes.com/singapore", "æœ¬åœ°æ–°é—»"),
        ("https://www.straitstimes.com/world", "å›½é™…æ–°é—»"),
        ("https://www.straitstimes.com/business", "ç»æµ"),
    ]
    for url, name in channels:
        crawl_channel(url, name)
    driver.quit()

# ========== è‡ªåŠ¨è°ƒåº¦ ==========
if __name__ == "__main__":
    while True:
        try:
            main()
            print("âœ… æ‰€æœ‰é¢‘é“çˆ¬å–å®Œæˆï¼Œè¿›å…¥ä¼‘çœ ")
            wait_until_next_6am()
        except KeyboardInterrupt:
            print("æ£€æµ‹åˆ°æ‰‹åŠ¨å…³é—­ï¼Œç¨‹åºé€€å‡ºã€‚")
            break
        except Exception as e:
            print(f"çˆ¬è™«å¼‚å¸¸ä¸­æ–­ï¼Œè‡ªåŠ¨é‡å¯ã€‚å¼‚å¸¸ä¿¡æ¯: {e}")
            traceback.print_exc()
            print("3ç§’åè‡ªåŠ¨é‡å¯...")
            time.sleep(3)
