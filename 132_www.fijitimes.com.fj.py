#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fiji Times çˆ¬è™« - æ¯è½®æ–°å‘ç°é“¾æ¥æ‰¹é‡çˆ¬å–å¹¶æŒ‰æ—¥æœŸåˆ†ç»„åˆå¹¶å­˜å‚¨
"""
import requests
from bs4 import BeautifulSoup, Tag
import json
import os
from datetime import datetime, timedelta
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from time import sleep
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import warnings
import logging
import shutil
from webdriver_manager.chrome import ChromeDriverManager  # è‡ªåŠ¨ç®¡ç†ChromeDriver

TXT_FILE = '132_fijitimes.txt'
JSON_DIR = 'data'

# æŠ‘åˆ¶è­¦å‘Šå’Œé”™è¯¯è¾“å‡º
warnings.filterwarnings("ignore")
logging.getLogger("selenium").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("requests").setLevel(logging.ERROR)

MONTH_MAP = {
    'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',
    'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',
    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
}

def load_titles():
    if not os.path.exists(TXT_FILE):
        return set()
    with open(TXT_FILE, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f if line.strip())

def save_title(title):
    with open(TXT_FILE, 'a', encoding='utf-8') as f:
        f.write(title + '\n')

def safe_publish_time(publish_time):
    today = datetime.now()
    m = re.search(r'Published: (\d+) hours? ago', publish_time)
    if m:
        return today.strftime('%Y-%m-%d')
    m = re.search(r'Published: (\d+) days? ago', publish_time)
    if m:
        days = int(m.group(1))
        dt = today - timedelta(days=days)
        return dt.strftime('%Y-%m-%d')
    m = re.search(r'Published: (\d+) weeks? ago', publish_time)
    if m:
        weeks = int(m.group(1))
        dt = today - timedelta(days=weeks*7)
        return dt.strftime('%Y-%m-%d')
    m = re.search(r'Published: ([A-Za-z]+) (\d{1,2}), (\d{4})', publish_time)
    if m:
        month = MONTH_MAP.get(m.group(1), '01')
        day = m.group(2).zfill(2)
        year = m.group(3)
        return f'{year}-{month}-{day}'
    pt = ''.join(filter(str.isdigit, publish_time))
    if len(pt) == 8:
        return f'{pt[:4]}-{pt[4:6]}-{pt[6:]}'
    return 'unknown'

def safe_filename(s):
    return re.sub(r'[^\w\u4e00-\u9fa5]', '', s)

def cleanup_chrome_temp():
    """å·²åºŸå¼ƒï¼Œä¸å†ä½¿ç”¨chrome_tempç›®å½•ï¼Œä¿ç•™ç©ºå®ç°é˜²æ­¢è°ƒç”¨æŠ¥é”™"""
    pass

def save_articles_grouped_by_date(articles, channel_name):
    """å°†åŒä¸€å¤©çš„æ–‡ç« åˆå¹¶å­˜ä¸ºä¸€ä¸ªjsonæ–‡ä»¶ï¼Œæ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨data/ä¸‹"""
    from collections import defaultdict
    grouped = defaultdict(list)
    for art in articles:
        date_str = art['metadata']['publish_time']
        grouped[date_str].append(art)
    now_str = datetime.now().strftime('%H%M%S')
    cat = safe_filename(channel_name)
    for date_str, arts in grouped.items():
        pt = date_str.replace('-', '')
        filename = f'132_{cat}_{pt}_{now_str}.json'
        filepath = os.path.join(JSON_DIR, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(arts, f, ensure_ascii=False, indent=2)
        print(f'ğŸ’¾ å·²ä¿å­˜{len(arts)}ç¯‡æ–‡ç« åˆ° {filepath}')

def crawl_article(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    for attempt in range(3):
        try:
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            title_elem = soup.find('h1', class_='fijitimes_title wp-block-post-title has-x-large-font-size')
            if not (isinstance(title_elem, Tag)):
                print(f"  Ã— æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
                return None, None, None
            title_text = title_elem.get_text(strip=True)
            content_elem = soup.find('div', class_='entry-content post_content wp-block-post-content is-layout-flow wp-block-post-content-is-layout-flow')
            if not (isinstance(content_elem, Tag)):
                print(f"  Ã— æœªæ‰¾åˆ°å†…å®¹å…ƒç´ ")
                return None, None, None
            content = '\n'.join([p.get_text(strip=True) for p in content_elem.find_all('p') if isinstance(p, Tag) and p.get_text(strip=True)])
            info_elem = soup.find('div', class_='fijitimes_post__info')
            publish_time, authors = '', ''
            if isinstance(info_elem, Tag):
                spans = [span for span in info_elem.find_all('span') if isinstance(span, Tag)]
                if len(spans) >= 2:
                    publish_time = spans[1].get_text(strip=True)
                if len(spans) >= 4:
                    authors = spans[3].get_text(strip=True)
            if authors.lower().startswith('by '):
                authors = authors[3:].strip()
            category = "ç»æµ"
            if '/local-news/' in url:
                category = "å½“åœ°æ–°é—»"
            elif '/world/' in url:
                category = "å›½é™…æ–°é—»"
            elif '/business/' in url:
                category = "ç»æµ"
            article_data = {
                "title": title_text,
                "content": content,
                "sources": {
                    "current_site": "æ¯æ—¥æ—¶æŠ¥",
                    "current_siteurl": "www.fijitimes.com.fj",
                    "origin_url": url
                },
                "metadata": {
                    "publish_time": safe_publish_time(publish_time),
                    "authors": authors,
                    "category": category
                },
                "crawlingtime": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            return article_data, title_text, publish_time
        except requests.exceptions.SSLError as e:
            print(f"  Ã— SSLé”™è¯¯ï¼Œé‡è¯•ç¬¬{attempt+1}æ¬¡: {url}")
            sleep(2)
            continue
        except Exception as e:
            print(f"  Ã— çˆ¬å–æ–‡ç« å¤±è´¥ {url}: {str(e)}")
            return None, None, None
    return None, None, None

def crawl_channel(channel_url, chromedriver_path=None):
    print(f"\nğŸŒ å¯åŠ¨æ— å¤´æµè§ˆå™¨åŠ è½½é¢‘é“: {channel_url}")
    
    # 1. ä¸ºæ¯ä¸ªçˆ¬è™«å®ä¾‹åˆ›å»ºç‹¬ç«‹çš„ä¸´æ—¶ç›®å½•ï¼Œé¿å…å†²çª
    import uuid
    unique_temp_dir = os.path.abspath(f'./chrome_temp_{uuid.uuid4().hex[:8]}')
    os.makedirs(unique_temp_dir, exist_ok=True)
    
    # 2. é…ç½®Chromeé€‰é¡¹ä¸ºæ— å¤´æ¨¡å¼ï¼Œæ·»åŠ åæ£€æµ‹åŠŸèƒ½
    chrome_options = Options()
    chrome_options.add_argument('--headless=new')  # ä½¿ç”¨æ–°ç‰ˆæ— å¤´æ¨¡å¼
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--no-first-run')
    chrome_options.add_argument('--no-default-browser-check')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument('--log-level=3')  # åªæ˜¾ç¤ºè‡´å‘½é”™è¯¯
    chrome_options.add_argument('--silent')  # é™é»˜æ¨¡å¼
    chrome_options.add_argument('--disable-logging')  # ç¦ç”¨æ—¥å¿—
    chrome_options.add_argument('--disable-web-security')  # ç¦ç”¨Webå®‰å…¨ï¼Œå¯èƒ½è§£å†³SSLé—®é¢˜
    chrome_options.add_argument('--ignore-ssl-errors')  # å¿½ç•¥SSLé”™è¯¯
    chrome_options.add_argument('--ignore-certificate-errors')  # å¿½ç•¥è¯ä¹¦é”™è¯¯
    chrome_options.add_argument(f'--user-data-dir={unique_temp_dir}')  # ä½¿ç”¨ç‹¬ç«‹ä¸´æ—¶ç›®å½•
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # 3. ä½¿ç”¨ä¼ å…¥çš„ChromeDriverè·¯å¾„æˆ–ä¸‹è½½æ–°çš„
    try:
        if chromedriver_path is None:
            # åªæœ‰ç¬¬ä¸€ä¸ªé¢‘é“éœ€è¦ä¸‹è½½ChromeDriver
            chromedriver_path = ChromeDriverManager().install()
            print(f"webdriver-managerä¸‹è½½/ä½¿ç”¨çš„ChromeDriverè·¯å¾„: {chromedriver_path}")
        else:
            print(f"å¤ç”¨å·²ä¸‹è½½çš„ChromeDriverè·¯å¾„: {chromedriver_path}")
        
        service = Service(chromedriver_path)
        driver = webdriver.Chrome(options=chrome_options, service=service)
        version = driver.capabilities.get('browserVersion') or driver.capabilities.get('version')
        print(f"å½“å‰Seleniumè°ƒç”¨çš„Chromeç‰ˆæœ¬: {version}")
        # è·å–å¹¶æ‰“å°ChromeDriverç‰ˆæœ¬
        chromedriver_version = driver.capabilities.get('chrome', {}).get('chromedriverVersion', 'æœªçŸ¥')
        print(f"å½“å‰Seleniumè°ƒç”¨çš„ChromeDriverç‰ˆæœ¬: {chromedriver_version}")
    except Exception as e:
        print(f"ChromeDriverä¸‹è½½æˆ–å¯åŠ¨å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(unique_temp_dir):
            try:
                shutil.rmtree(unique_temp_dir)
            except:
                pass
        return
    
    # æ‰§è¡ŒJavaScriptæ¥éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
    driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})")
    
    try:
        driver.get(channel_url)
    except Exception as e:
        print(f'âš ï¸ driver.get({channel_url}) å¤±è´¥: {e}')
        # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šé‡è¯•3æ¬¡
        retry_count = 0
        max_retries = 3
        while retry_count < max_retries:
            retry_count += 1
            print(f'ğŸ”„ é‡è¯•ç¬¬{retry_count}æ¬¡è®¿é—®é¢‘é“: {channel_url}')
            try:
                sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                driver.get(channel_url)
                print(f'âœ… é‡è¯•æˆåŠŸï¼Œç»§ç»­çˆ¬å–')
                break
            except Exception as retry_e:
                print(f'âŒ é‡è¯•ç¬¬{retry_count}æ¬¡å¤±è´¥: {retry_e}')
                if retry_count >= max_retries:
                    print(f'âš ï¸ è¿ç»­{max_retries}æ¬¡è®¿é—®å¤±è´¥ï¼Œè·³è¿‡å½“å‰é¢‘é“')
                    try:
                        driver.quit()
                        print("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
                    except:
                        pass
                    # æ¸…ç†ä¸´æ—¶ç›®å½•å’ŒChromeDriverç¼“å­˜
                    try:
                        if os.path.exists(unique_temp_dir):
                            shutil.rmtree(unique_temp_dir)
                            print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•")
                        if os.path.exists('./chromedriver_cache'):
                            shutil.rmtree('./chromedriver_cache')
                            print("ğŸ§¹ å·²æ¸…ç†ChromeDriverç¼“å­˜ç›®å½•")
                    except Exception as cleanup_e:
                        print(f"âš ï¸ æ¸…ç†ç›®å½•å¤±è´¥: {cleanup_e}")
                    return
                continue
    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    sleep(3)
    
    max_clicks = 100
    click_count = 0
    seen_links = set()
    titles_set = load_titles()
    print(f"å·²åŠ è½½ {len(titles_set)} ä¸ªå†å²æ ‡é¢˜ç”¨äºå»é‡")
    # é¢‘é“åç›´æ¥æ ¹æ®å…¥å£URLåˆ¤æ–­
    if '/local-news/' in channel_url:
        channel_name = 'å½“åœ°æ–°é—»'
    elif '/world/' in channel_url:
        channel_name = 'å›½é™…æ–°é—»'
    elif '/business/' in channel_url:
        channel_name = 'ç»æµ'
    else:
        channel_name = 'æœªçŸ¥é¢‘é“'
    
    # ç”¨äºä¸­æ–­ä¿å­˜çš„å˜é‡
    all_articles = []
    no_loadmore_count = 0  # è¿ç»­æœªæ£€æµ‹åˆ°Load moreæŒ‰é’®çš„è®¡æ•°å™¨
    no_loadmore_threshold = 15
    try:
        while click_count < max_clicks:
            print(f"\n--- ç¬¬ {click_count + 1} æ¬¡åŠ è½½ ---")
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            links = soup.find_all('a', class_='ps-no-underline ps-leading-tight ps-text-blockBlack')
            urls = []
            for link in links:
                if isinstance(link, Tag):
                    href = link.get('href')
                    if isinstance(href, str) and href.startswith('http'):
                        urls.append(href)
            urls = list(set(urls))
            new_urls = [u for u in urls if u not in seen_links]
            print(f'æœ¬è½®æ–°å‘ç° {len(new_urls)} ä¸ªé“¾æ¥')
            # æ‰¹é‡çˆ¬å–æœ¬è½®æ‰€æœ‰æ–°é“¾æ¥
            articles_this_round = []
            for url in new_urls:
                seen_links.add(url)
                article_data, title_text, publish_time = crawl_article(url)
                if not article_data or not title_text:
                    continue
                if title_text in titles_set:
                    print(f'  Ã— å·²çˆ¬å–è¿‡: {title_text}')
                    continue
                articles_this_round.append(article_data)
                save_title(title_text)
                titles_set.add(title_text)
                print(f'  âœ… æ–°æ–‡ç« : {title_text}')
                sleep(1.5)
            # æœ¬è½®æ‰€æœ‰æ–°æ–‡ç« æŒ‰æ—¥æœŸåˆ†ç»„å­˜å‚¨
            if not os.path.exists(JSON_DIR):
                os.makedirs(JSON_DIR)
            if articles_this_round:
                save_articles_grouped_by_date(articles_this_round, channel_name)
                all_articles.extend(articles_this_round)
            # è¿ç»­5æ¬¡æœªæ£€æµ‹åˆ°Load moreæ‰break
            try:
                load_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.XPATH, "//a[contains(@class, 'ps-cursor-pointer') and contains(., 'Load more')]"))
                )
                no_loadmore_count = 0  # æ£€æµ‹åˆ°æŒ‰é’®ï¼Œé‡ç½®è®¡æ•°
            except:
                no_loadmore_count += 1
                print(f"æœªæ‰¾åˆ°'Load more'æŒ‰é’®ï¼Œç´¯è®¡{no_loadmore_count}æ¬¡")
                if no_loadmore_count >= no_loadmore_threshold:
                    print(f"è¿ç»­{no_loadmore_threshold}æ¬¡æœªæ£€æµ‹åˆ°'Load more'æŒ‰é’®ï¼Œé¢‘é“å¯èƒ½å·²åŠ è½½å…¨éƒ¨å†…å®¹")
                    break
                else:
                    sleep(2)
                    continue
            # ç‚¹å‡»æŒ‰é’®å‰å…ˆæ»šåŠ¨åˆ°å¯è§åŒºåŸŸï¼Œå¤±è´¥é‡è¯•3æ¬¡
            click_success = False
            for click_attempt in range(3):
                try:
                    driver.execute_script("arguments[0].scrollIntoView(true);", load_btn)
                    sleep(0.5)
                    load_btn.click()
                    click_count += 1
                    print(f"ç‚¹å‡»'Load more'æŒ‰é’® ({click_count}/{max_clicks})")
                    sleep(2)
                    try:
                        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
                        print("å·²æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨")
                        sleep(1)
                    except:
                        print("æ»šåŠ¨å¤±è´¥ï¼Œç»§ç»­å¤„ç†")
                    click_success = True
                    break
                except Exception as e:
                    print(f"ç‚¹å‡»æŒ‰é’®å¤±è´¥ï¼ˆç¬¬{click_attempt+1}æ¬¡ï¼‰: {str(e)}")
                    sleep(1)
            if not click_success:
                print(f"è¿ç»­3æ¬¡ç‚¹å‡»'Load more'æŒ‰é’®å¤±è´¥ï¼Œè·³å‡ºå¾ªç¯")
                break
    except KeyboardInterrupt:
        print("\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰ï¼Œæ­£åœ¨ä¿å­˜å·²çˆ¬å–å†…å®¹...")
    finally:
        # æ— è®ºå¦‚ä½•éƒ½ä¿å­˜ä¸€æ¬¡
        if all_articles:
            print(f"\nâš ï¸ æ­£åœ¨ä¿å­˜å·²çˆ¬å–çš„{len(all_articles)}ç¯‡æ–‡ç« ...")
            save_articles_grouped_by_date(all_articles, channel_name)
        else:
            print("\nâš ï¸ æ²¡æœ‰éœ€è¦é¢å¤–ä¿å­˜çš„æ–‡ç« ã€‚")
        try:
            driver.quit()
            print("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
        except:
            pass
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•å’ŒChromeDriverç¼“å­˜
        try:
            if os.path.exists(unique_temp_dir):
                shutil.rmtree(unique_temp_dir)
                print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½•")
            if os.path.exists('./chromedriver_cache'):
                shutil.rmtree('./chromedriver_cache')
                print("ğŸ§¹ å·²æ¸…ç†ChromeDriverç¼“å­˜ç›®å½•")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ç›®å½•å¤±è´¥: {e}")

def main():
    print("ğŸ¯ Fiji Times é¢‘é“é€æ­¥çˆ¬è™«å¯åŠ¨")
    
    # å…ˆè®¾ç½®webdriver-managerç¯å¢ƒå˜é‡
    os.environ['WDM_MIRROR'] = 'https://registry.npmmirror.com/-/binary/chromedriver'
    os.environ['WDM_CACHE_PATH'] = os.path.abspath('./chromedriver_cache')
    os.environ['WDM_LOCAL'] = '0'
    os.environ['WDM_SSL_VERIFY'] = 'false'
    
    channels = [
        "https://www.fijitimes.com.fj/category/news/business/",
        "https://www.fijitimes.com.fj/category/news/local-news/",
        "https://www.fijitimes.com.fj/category/news/world/"
    ]
    
    # å…ˆä¸‹è½½ChromeDriverï¼Œä¾›æ‰€æœ‰é¢‘é“ä½¿ç”¨ï¼ˆæ·»åŠ é‡è¯•æœºåˆ¶ï¼‰
    chromedriver_path = None
    max_retries = 3
    for retry_count in range(max_retries):
        try:
            print(f"ğŸ”§ æ­£åœ¨ä¸‹è½½ChromeDriver... (ç¬¬{retry_count + 1}æ¬¡å°è¯•)")
            chromedriver_path = ChromeDriverManager().install()
            print(f"âœ… ChromeDriverä¸‹è½½å®Œæˆ: {chromedriver_path}")
            break
        except Exception as e:
            print(f"âŒ ChromeDriverä¸‹è½½å¤±è´¥ (ç¬¬{retry_count + 1}æ¬¡): {e}")
            if retry_count < max_retries - 1:
                print("ğŸ”„ ç­‰å¾…5ç§’åé‡è¯•...")
                sleep(5)
                # æ¸…ç†å¯èƒ½æŸåçš„ç¼“å­˜
                try:
                    if os.path.exists('./chromedriver_cache'):
                        shutil.rmtree('./chromedriver_cache')
                        print("ğŸ§¹ å·²æ¸…ç†æŸåçš„ChromeDriverç¼“å­˜")
                except:
                    pass
            else:
                print(f"âŒ è¿ç»­{max_retries}æ¬¡ä¸‹è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
                return
    
    try:
        for i, channel_url in enumerate(channels):
            try:
                print(f"\nğŸ“º å¼€å§‹çˆ¬å–ç¬¬{i+1}ä¸ªé¢‘é“: {channel_url}")
                crawl_channel(channel_url, chromedriver_path)
            except KeyboardInterrupt:
                print("\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰ï¼Œç¨‹åºç›´æ¥é€€å‡º")
                return
        print("\nğŸ¯ æ‰€æœ‰é¢‘é“çˆ¬å–å®Œæˆï¼")
    except KeyboardInterrupt:
        print("\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰ï¼Œç¨‹åºç›´æ¥é€€å‡º")
        return
    finally:
        cleanup_chrome_temp()
        # æ¸…ç†ChromeDriverç¼“å­˜ç›®å½•
        try:
            if os.path.exists('./chromedriver_cache'):
                shutil.rmtree('./chromedriver_cache')
                print("ğŸ§¹ å·²æ¸…ç†ChromeDriverç¼“å­˜ç›®å½•")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ChromeDriverç¼“å­˜å¤±è´¥: {e}")

if __name__ == '__main__':
    main() 