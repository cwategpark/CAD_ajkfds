#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fiji Times çˆ¬è™« - æ¯è½®æ–°å‘ç°é“¾æ¥æ‰¹é‡çˆ¬å–å¹¶æŒ‰æ—¥æœŸåˆ†ç»„åˆå¹¶å­˜å‚¨
"""
import requests
from bs4 import BeautifulSoup, Tag
import json
import os
from datetime import datetime, timedelta
import sys
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from time import sleep
import re
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import warnings
import logging
import shutil

TXT_FILE = '132_fijitimes.txt'
JSON_DIR = 'data'

# æŠ‘åˆ¶è­¦å‘Šå’Œé”™è¯¯è¾“å‡º
warnings.filterwarnings("ignore")
logging.getLogger("selenium").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("requests").setLevel(logging.ERROR)

MONTH_MAP = {
    'January': '01', 'February': '02', 'March': '03', 'April': '04', 'May': '05', 'June': '06',
    'July': '07', 'August': '08', 'September': '09', 'October': '10', 'November': '11', 'December': '12',
    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
}

def load_titles():
    if not os.path.exists(TXT_FILE):
        return set()
    with open(TXT_FILE, 'r', encoding='utf-8') as f:
        return set(line.strip() for line in f if line.strip())

def save_title(title):
    with open(TXT_FILE, 'a', encoding='utf-8') as f:
        f.write(title + '\n')

def safe_publish_time(publish_time):
    today = datetime.now()
    m = re.search(r'Published: (\d+) hours? ago', publish_time)
    if m:
        return today.strftime('%Y-%m-%d')
    m = re.search(r'Published: (\d+) days? ago', publish_time)
    if m:
        days = int(m.group(1))
        dt = today - timedelta(days=days)
        return dt.strftime('%Y-%m-%d')
    m = re.search(r'Published: (\d+) weeks? ago', publish_time)
    if m:
        weeks = int(m.group(1))
        dt = today - timedelta(days=weeks*7)
        return dt.strftime('%Y-%m-%d')
    m = re.search(r'Published: ([A-Za-z]+) (\d{1,2}), (\d{4})', publish_time)
    if m:
        month = MONTH_MAP.get(m.group(1), '01')
        day = m.group(2).zfill(2)
        year = m.group(3)
        return f'{year}-{month}-{day}'
    pt = ''.join(filter(str.isdigit, publish_time))
    if len(pt) == 8:
        return f'{pt[:4]}-{pt[4:6]}-{pt[6:]}'
    return 'unknown'

def safe_filename(s):
    return re.sub(r'[^\w\u4e00-\u9fa5]', '', s)

def cleanup_chrome_temp():
    """æ¸…ç†Chromeä¸´æ—¶ç›®å½•"""
    temp_dir = './chrome_temp'
    if os.path.exists(temp_dir):
        try:
            shutil.rmtree(temp_dir)
            print("ğŸ§¹ å·²æ¸…ç†Chromeä¸´æ—¶ç›®å½•")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

def save_articles_grouped_by_date(articles, channel_name):
    """å°†åŒä¸€å¤©çš„æ–‡ç« åˆå¹¶å­˜ä¸ºä¸€ä¸ªjsonæ–‡ä»¶ï¼Œæ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨data/ä¸‹"""
    from collections import defaultdict
    grouped = defaultdict(list)
    for art in articles:
        date_str = art['metadata']['publish_time']
        grouped[date_str].append(art)
    now_str = datetime.now().strftime('%H%M%S')
    cat = safe_filename(channel_name)
    for date_str, arts in grouped.items():
        pt = date_str.replace('-', '')
        filename = f'132_{cat}_{pt}_{now_str}.json'
        filepath = os.path.join(JSON_DIR, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(arts, f, ensure_ascii=False, indent=2)
        print(f'ğŸ’¾ å·²ä¿å­˜{len(arts)}ç¯‡æ–‡ç« åˆ° {filepath}')

def crawl_article(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    for attempt in range(3):
        try:
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            title_elem = soup.find('h1', class_='fijitimes_title wp-block-post-title has-x-large-font-size')
            if not (isinstance(title_elem, Tag)):
                print(f"  Ã— æœªæ‰¾åˆ°æ ‡é¢˜å…ƒç´ ")
                return None, None, None
            title_text = title_elem.get_text(strip=True)
            content_elem = soup.find('div', class_='entry-content post_content wp-block-post-content is-layout-flow wp-block-post-content-is-layout-flow')
            if not (isinstance(content_elem, Tag)):
                print(f"  Ã— æœªæ‰¾åˆ°å†…å®¹å…ƒç´ ")
                return None, None, None
            content = '\n'.join([p.get_text(strip=True) for p in content_elem.find_all('p') if isinstance(p, Tag) and p.get_text(strip=True)])
            info_elem = soup.find('div', class_='fijitimes_post__info')
            publish_time, authors = '', ''
            if isinstance(info_elem, Tag):
                spans = [span for span in info_elem.find_all('span') if isinstance(span, Tag)]
                if len(spans) >= 2:
                    publish_time = spans[1].get_text(strip=True)
                if len(spans) >= 4:
                    authors = spans[3].get_text(strip=True)
            if authors.lower().startswith('by '):
                authors = authors[3:].strip()
            category = "ç»æµ"
            if '/local-news/' in url:
                category = "å½“åœ°æ–°é—»"
            elif '/world/' in url:
                category = "å›½é™…æ–°é—»"
            elif '/business/' in url:
                category = "ç»æµ"
            article_data = {
                "title": title_text,
                "content": content,
                "sources": {
                    "current_site": "æ¯æ—¥æ—¶æŠ¥",
                    "current_siteurl": "www.fijitimes.com.fj",
                    "origin_url": url
                },
                "metadata": {
                    "publish_time": safe_publish_time(publish_time),
                    "authors": authors,
                    "category": category
                },
                "crawlingtime": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            return article_data, title_text, publish_time
        except requests.exceptions.SSLError as e:
            print(f"  Ã— SSLé”™è¯¯ï¼Œé‡è¯•ç¬¬{attempt+1}æ¬¡: {url}")
            sleep(2)
            continue
        except Exception as e:
            print(f"  Ã— çˆ¬å–æ–‡ç« å¤±è´¥ {url}: {str(e)}")
            return None, None, None
    return None, None, None

def crawl_channel(channel_url):
    print(f"\nğŸŒ å¯åŠ¨æ— å¤´æµè§ˆå™¨åŠ è½½é¢‘é“: {channel_url}")
    
    # 1. è‡ªåŠ¨æ¸…ç†chrome_tempç›®å½•ï¼Œå¹¶ç”¨ç»å¯¹è·¯å¾„
    chrome_temp_dir = os.path.abspath('./chrome_temp')
    if os.path.exists(chrome_temp_dir):
        try:
            shutil.rmtree(chrome_temp_dir)
        except Exception as e:
            print(f"æ— æ³•åˆ é™¤æ—§çš„chrome_tempç›®å½•: {e}")
    os.makedirs(chrome_temp_dir, exist_ok=True)
    
    # 2. é…ç½®Chromeé€‰é¡¹ä¸ºæ— å¤´æ¨¡å¼ï¼Œæ·»åŠ åæ£€æµ‹åŠŸèƒ½
    chrome_options = Options()
    chrome_options.add_argument('--headless=new')  # ä½¿ç”¨æ–°ç‰ˆæ— å¤´æ¨¡å¼
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_argument('--log-level=3')  # åªæ˜¾ç¤ºè‡´å‘½é”™è¯¯
    chrome_options.add_argument('--silent')  # é™é»˜æ¨¡å¼
    chrome_options.add_argument('--disable-logging')  # ç¦ç”¨æ—¥å¿—
    chrome_options.add_argument('--disable-web-security')  # ç¦ç”¨Webå®‰å…¨ï¼Œå¯èƒ½è§£å†³SSLé—®é¢˜
    chrome_options.add_argument('--ignore-ssl-errors')  # å¿½ç•¥SSLé”™è¯¯
    chrome_options.add_argument('--ignore-certificate-errors')  # å¿½ç•¥è¯ä¹¦é”™è¯¯
    chrome_options.add_argument(f'--user-data-dir={chrome_temp_dir}')  # ä½¿ç”¨ç»å¯¹è·¯å¾„
    chrome_options.add_argument('--no-first-run')  # è·³è¿‡é¦–æ¬¡è¿è¡Œè®¾ç½®
    chrome_options.add_argument('--no-default-browser-check')  # ä¸æ£€æŸ¥é»˜è®¤æµè§ˆå™¨
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(log_output=os.devnull)  # å°†Chromeæ—¥å¿—è¾“å‡ºåˆ°ç©ºè®¾å¤‡
    driver = webdriver.Chrome(options=chrome_options, service=service)
    
    # æ‰§è¡ŒJavaScriptæ¥éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    driver.execute_script("Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})")
    driver.execute_script("Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})")
    
    try:
        driver.get(channel_url)
    except Exception as e:
        print(f"âš ï¸ driver.get({channel_url}) å¤±è´¥: {e}")
        try:
            driver.quit()
        except:
            pass
        return
    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    sleep(3)
    
    max_clicks = 100
    click_count = 0
    seen_links = set()
    titles_set = load_titles()
    print(f"å·²åŠ è½½ {len(titles_set)} ä¸ªå†å²æ ‡é¢˜ç”¨äºå»é‡")
    # é¢‘é“åç›´æ¥æ ¹æ®å…¥å£URLåˆ¤æ–­
    if '/local-news/' in channel_url:
        channel_name = 'å½“åœ°æ–°é—»'
    elif '/world/' in channel_url:
        channel_name = 'å›½é™…æ–°é—»'
    elif '/business/' in channel_url:
        channel_name = 'ç»æµ'
    else:
        channel_name = 'æœªçŸ¥é¢‘é“'
    
    # ç”¨äºä¸­æ–­ä¿å­˜çš„å˜é‡
    all_articles = []
    no_loadmore_count = 0  # è¿ç»­æœªæ£€æµ‹åˆ°Load moreæŒ‰é’®çš„è®¡æ•°å™¨
    no_loadmore_threshold = 15
    try:
        while click_count < max_clicks:
            print(f"\n--- ç¬¬ {click_count + 1} æ¬¡åŠ è½½ ---")
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            links = soup.find_all('a', class_='ps-no-underline ps-leading-tight ps-text-blockBlack')
            urls = []
            for link in links:
                if isinstance(link, Tag):
                    href = link.get('href')
                    if isinstance(href, str) and href.startswith('http'):
                        urls.append(href)
            urls = list(set(urls))
            new_urls = [u for u in urls if u not in seen_links]
            print(f'æœ¬è½®æ–°å‘ç° {len(new_urls)} ä¸ªé“¾æ¥')
            # æ‰¹é‡çˆ¬å–æœ¬è½®æ‰€æœ‰æ–°é“¾æ¥
            articles_this_round = []
            for url in new_urls:
                seen_links.add(url)
                article_data, title_text, publish_time = crawl_article(url)
                if not article_data or not title_text:
                    continue
                if title_text in titles_set:
                    print(f'  Ã— å·²çˆ¬å–è¿‡: {title_text}')
                    continue
                articles_this_round.append(article_data)
                save_title(title_text)
                titles_set.add(title_text)
                print(f'  âœ… æ–°æ–‡ç« : {title_text}')
                sleep(1.5)
            # æœ¬è½®æ‰€æœ‰æ–°æ–‡ç« æŒ‰æ—¥æœŸåˆ†ç»„å­˜å‚¨
            if not os.path.exists(JSON_DIR):
                os.makedirs(JSON_DIR)
            if articles_this_round:
                save_articles_grouped_by_date(articles_this_round, channel_name)
                all_articles.extend(articles_this_round)
            # è¿ç»­5æ¬¡æœªæ£€æµ‹åˆ°Load moreæ‰break
            try:
                load_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.XPATH, "//a[contains(@class, 'ps-cursor-pointer') and contains(., 'Load more')]"))
                )
                no_loadmore_count = 0  # æ£€æµ‹åˆ°æŒ‰é’®ï¼Œé‡ç½®è®¡æ•°
            except:
                no_loadmore_count += 1
                print(f"æœªæ‰¾åˆ°'Load more'æŒ‰é’®ï¼Œç´¯è®¡{no_loadmore_count}æ¬¡")
                if no_loadmore_count >= no_loadmore_threshold:
                    print(f"è¿ç»­{no_loadmore_threshold}æ¬¡æœªæ£€æµ‹åˆ°'Load more'æŒ‰é’®ï¼Œé¢‘é“å¯èƒ½å·²åŠ è½½å…¨éƒ¨å†…å®¹")
                    break
                else:
                    sleep(2)
                    continue
            # ç‚¹å‡»æŒ‰é’®å‰å…ˆæ»šåŠ¨åˆ°å¯è§åŒºåŸŸï¼Œå¤±è´¥é‡è¯•3æ¬¡
            click_success = False
            for click_attempt in range(3):
                try:
                    driver.execute_script("arguments[0].scrollIntoView(true);", load_btn)
                    sleep(0.5)
                    load_btn.click()
                    click_count += 1
                    print(f"ç‚¹å‡»'Load more'æŒ‰é’® ({click_count}/{max_clicks})")
                    sleep(2)
                    try:
                        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
                        print("å·²æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨")
                        sleep(1)
                    except:
                        print("æ»šåŠ¨å¤±è´¥ï¼Œç»§ç»­å¤„ç†")
                    click_success = True
                    break
                except Exception as e:
                    print(f"ç‚¹å‡»æŒ‰é’®å¤±è´¥ï¼ˆç¬¬{click_attempt+1}æ¬¡ï¼‰: {str(e)}")
                    sleep(1)
            if not click_success:
                print(f"è¿ç»­3æ¬¡ç‚¹å‡»'Load more'æŒ‰é’®å¤±è´¥ï¼Œè·³å‡ºå¾ªç¯")
                break
    except KeyboardInterrupt:
        print("\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼ˆCtrl+Cï¼‰ï¼Œæ­£åœ¨ä¿å­˜å·²çˆ¬å–å†…å®¹...")
    finally:
        # æ— è®ºå¦‚ä½•éƒ½ä¿å­˜ä¸€æ¬¡
        if all_articles:
            print(f"\nâš ï¸ æ­£åœ¨ä¿å­˜å·²çˆ¬å–çš„{len(all_articles)}ç¯‡æ–‡ç« ...")
            save_articles_grouped_by_date(all_articles, channel_name)
        else:
            print("\nâš ï¸ æ²¡æœ‰éœ€è¦é¢å¤–ä¿å­˜çš„æ–‡ç« ã€‚")
        try:
            driver.quit()
            print("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
        except:
            pass

def main():
    print("ğŸ¯ Fiji Times é¢‘é“é€æ­¥çˆ¬è™«å¯åŠ¨")
    channels = [
        "https://www.fijitimes.com.fj/category/news/business/",
        "https://www.fijitimes.com.fj/category/news/local-news/",
        "https://www.fijitimes.com.fj/category/news/world/"
    ]
    try:
        for channel_url in channels:
            crawl_channel(channel_url)
        print("\nğŸ¯ æ‰€æœ‰é¢‘é“çˆ¬å–å®Œæˆï¼")
    finally:
        cleanup_chrome_temp()

if __name__ == '__main__':
    main() 